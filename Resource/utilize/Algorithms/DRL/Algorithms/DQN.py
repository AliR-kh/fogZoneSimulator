import numpy as np

# ==============================================================================
# قالب کلاس محیط سفارشی شما
# ==============================================================================
class MyCustomEnv:
    def __init__(self, # پارامترهای ورودی محیط خودتان را اینجا تعریف کنید
                 num_actions=5, 
                 state_vector_size=5):
        """
        سازنده محیط شما.
        تمام تنظیمات اولیه مانند تعداد منابع، ویژگی‌های تسک و غیره اینجا انجام می‌شود.
        """
        self.action_size = num_actions
        self.state_size = state_vector_size
        
        # *** بسیار مهم ***
        # یک مقدار منطقی برای ماکسیمم مقدار یک عنصر در وکتور وضعیت خام تعریف کنید.
        # این مقدار برای نرمال‌سازی استفاده خواهد شد.
        # برای مثال، اگر وضعیت شما بار پردازنده است و می‌دانید هیچگاه از 1000 بالاتر نمی‌رود،
        # می‌توانید این مقدار را 1000.0 در نظر بگیرید.
        self.max_state_value = 1000.0  # <--- این مقدار را با توجه به محیط خودتان تنظیم کنید

        print("محیط سفارشی من ساخته شد!")

    def reset(self):
        """
        محیط را برای یک اپیزود جدید آماده می‌کند.
        باید وضعیت اولیه خام (un-normalized) را برگرداند.
        """
        # منطق ریست کردن محیط شما در اینجا قرار می‌گیرد
        # برای مثال، ریست کردن بار منابع، صفر کردن شمارنده‌ها و ...
        
        # به عنوان مثال، یک وضعیت اولیه تصادفی برمی‌گردانیم
        initial_state_raw = np.zeros(self.state_size, dtype=np.float32)
        print("محیط ریست شد.")
        return initial_state_raw

    def step(self, action):
        """
        یک اقدام را اجرا کرده و نتیجه را برمی‌گرداند.
        باید یک تاپل (next_state_raw, reward, done, info) برگرداند.
        """
        # منطق اصلی محیط شما اینجا قرار می‌گیرد
        # 1. بر اساس 'action' دریافتی، وضعیت داخلی محیط را تغییر دهید.
        # 2. پاداش (reward) را محاسبه کنید.
        # 3. بررسی کنید که آیا اپیزود تمام شده است یا نه (done).
        # 4. وضعیت جدید خام (next_state_raw) را ایجاد کنید.
        
        # برای مثال:
        next_state_raw = np.random.rand(self.state_size).astype(np.float32) # وضعیت بعدی فرضی
        reward = np.random.uniform(-1, 1) # پاداش فرضی
        done = False # آیا اپیزود تمام شده؟
        info = {} # اطلاعات اضافی (اختیاری)
        
        return next_state_raw, reward, done, info

    def get_state_size(self):
        """اندازه وکتور وضعیت را برمی‌گرداند."""
        return self.state_size

    def get_action_size(self):
        """تعداد اقدامات ممکن را برمی‌گرداند."""
        return self.action_size